{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2023-03-24 13:18:18,097 -     INFO - Importing libraries... (2238536406.py:2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from log import logger\n",
    "logger.info('Importing libraries...')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import the neccessary module\n",
    "from helper import Model\n",
    "from helper import Auxiliary\n",
    "# Modelling\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.backend import clear_session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2023-03-24 13:24:16,169 -     INFO - Reading data... (910505981.py:2)\u001b[0m\n",
      "\u001b[38;5;39m2023-03-24 13:24:16,338 -     INFO - Selecting columns:'case:concept:name', 'concept:name', 'time:timestamp', 'Next Time', 'lifecycle:transition' (910505981.py:7)\u001b[0m\n",
      "\u001b[38;5;39m2023-03-24 13:24:16,344 -     INFO - Removing NaN values... (910505981.py:12)\u001b[0m\n",
      "\u001b[38;5;39m2023-03-24 13:24:16,356 -     INFO - Eliminating outliers... (910505981.py:19)\u001b[0m\n",
      "\u001b[38;5;39m2023-03-24 13:24:16,364 -     INFO - One hot encoding of the activities... (910505981.py:31)\u001b[0m\n",
      "\u001b[38;5;39m2023-03-24 13:24:16,489 -     INFO - Pulling the week day from the timestamp and hot encoding it... (910505981.py:38)\u001b[0m\n",
      "\u001b[38;5;39m2023-03-24 13:24:16,953 -     INFO - Dropping timestamp... (910505981.py:52)\u001b[0m\n",
      "\u001b[38;5;39m2023-03-24 13:24:16,958 -     INFO - Encoding lifecycle:transition... (910505981.py:57)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#df_train, df_test = Auxiliary.train_test_split(Auxiliary.preprocess_data(Model.get_csv('BPI_Challenge_2012.csv')))\n",
    "logger.info('Reading data...')\n",
    "df_train = Model.get_csv('train.csv')\n",
    "df_test = Model.get_csv('test.csv')\n",
    "\n",
    "#only select needed columns\n",
    "logger.info(\"Selecting columns:'case:concept:name', 'concept:name', 'time:timestamp', 'Next Time', 'lifecycle:transition'\")\n",
    "df_train = df_train[['case:concept:name', 'concept:name', 'time:timestamp', 'Next Time', 'lifecycle:transition']]\n",
    "df_test = df_test[['case:concept:name', 'concept:name', 'time:timestamp', 'Next Time', 'lifecycle:transition']]\n",
    "\n",
    "#remove entries where there is NaN\n",
    "logger.info('Removing NaN values...')\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "df_train = df_train.replace(-1, 0)\n",
    "df_test = df_test.replace(-1, 0)\n",
    "\n",
    "#eliminate outliers\n",
    "logger.info('Eliminating outliers...')\n",
    "#q__train_low = df_train[\"Next Time\"].quantile(0.03)\n",
    "q__train_hi  = df_train[\"Next Time\"].quantile(0.97)\n",
    "\n",
    "#df_train = df_train[(df_train[\"Next Time\"] < q__train_hi) & (df_train[\"Next Time\"] > q__train_low)]\n",
    "df_train = df_train[df_train[\"Next Time\"] < q__train_hi]\n",
    "#q_test_low = df_test[\"Next Time\"].quantile(0.01)\n",
    "q_test_hi = df_test[\"Next Time\"].quantile(0.97)\n",
    "#df_test = df_test[(df_test[\"Next Time\"] < q_test_hi) & (df_test[\"Next Time\"] > q_test_low)]\n",
    "df_test = df_test[df_test[\"Next Time\"] < q_test_hi]\n",
    "\n",
    "#One hot encoding of the activities\n",
    "logger.info('One hot encoding of the activities...')\n",
    "df_train['concept:name'] = df_train['concept:name'].apply(lambda x: Auxiliary.one_hot_encode(x, 24))\n",
    "df_test['concept:name'] = df_test['concept:name'].apply(lambda x: Auxiliary.one_hot_encode(x, 24))\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "#get week day from timestamp and hot encode it\n",
    "logger.info('Pulling the week day from the timestamp and hot encoding it...')\n",
    "df_train['time:timestamp'] = pd.to_datetime(df_train['time:timestamp'])\n",
    "\n",
    "df_train['Week Day'] = df_train['time:timestamp'].apply(lambda x: x.strftime('%w'))\n",
    "\n",
    "df_train['Week Day'] = df_train['Week Day'].apply(lambda x: Auxiliary.one_hot_encode(int(x), 7))\n",
    "\n",
    "df_test['time:timestamp'] = pd.to_datetime(df_test['time:timestamp'])\n",
    "\n",
    "df_test['Week Day'] = df_test['time:timestamp'].apply(lambda x: int(x.strftime('%w')))\n",
    "\n",
    "df_test['Week Day'] = df_test['Week Day'].apply(lambda x: Auxiliary.one_hot_encode(int(x), 7))\n",
    "\n",
    "#drop timestamp\n",
    "logger.info('Dropping timestamp...')\n",
    "df_train.drop(['time:timestamp'], axis=1, inplace=True)\n",
    "df_test.drop(['time:timestamp'], axis=1, inplace=True)\n",
    "\n",
    "#encode lifecycle:transition\n",
    "logger.info('Encoding lifecycle:transition...')\n",
    "df_train['lifecycle:transition'] = df_train['lifecycle:transition'].apply(lambda x: Auxiliary.one_hot_encode(x, df_train['lifecycle:transition'].nunique()))\n",
    "df_test['lifecycle:transition'] = df_test['lifecycle:transition'].apply(lambda x: Auxiliary.one_hot_encode(x, df_test['lifecycle:transition'].nunique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the concept:name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2023-03-24 13:24:37,701 -     INFO - Splitting data into x and y... (4105737707.py:2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#x_train = df_train[['org:resource', 'lifecycle:transition','concept:name','case:AMOUNT_REQ','month', 'day']]\n",
    "logger.info('Splitting data into x and y...')\n",
    "x_train = df_train[['concept:name', 'lifecycle:transition', 'Week Day']]\n",
    "y_train = df_train[['Next Time']]\n",
    "#x_test = df_test[['org:resource', 'lifecycle:transition','concept:name','case:AMOUNT_REQ','month', 'day']]\n",
    "x_test = df_test[['concept:name' , 'lifecycle:transition', 'Week Day']]\n",
    "y_test = df_test[['Next Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37696, 3)\n",
      "(37696, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing y values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2023-03-24 13:24:44,265 -     INFO - Normalizing y values... (700907669.py:4)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#normalize y values\n",
    "\n",
    "split_location = y_train.shape[0]\n",
    "logger.info('Normalizing y values...')\n",
    "y_df = pd.concat([y_train, y_test])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(y_df)\n",
    "\n",
    "y_df = scaler.transform(y_df)\n",
    "\n",
    "#Resplit as numpy arrays\n",
    "y_train = y_df[0:split_location]\n",
    "y_test = y_df[split_location:]\n",
    "\n",
    "x_train = x_train.values\n",
    "x_test = x_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37696, 3) (37696, 1) (16684, 3) (16684, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2023-03-24 13:24:48,175 -     INFO - Reshaping x values from 3D to 2D... (789070195.py:1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info('Reshaping x values from 3D to 2D...')\n",
    "temp_array = []\n",
    "real_x_train = []\n",
    "real_x_test = []\n",
    "for index,value in enumerate(x_train):\n",
    "    temp_array = []\n",
    "    temp_array = value[0]\n",
    "    temp_array = np.append(temp_array, value[1])\n",
    "    temp_array = np.append(temp_array, value[2])\n",
    "    real_x_train.append(temp_array)\n",
    "for index,value in enumerate(x_test):\n",
    "    temp_array = []\n",
    "    temp_array = value[0]\n",
    "    temp_array = np.append(temp_array, value[1])\n",
    "    temp_array = np.append(temp_array, value[2])\n",
    "    real_x_test.append(temp_array)\n",
    "\n",
    "x_train = np.array(real_x_train)\n",
    "x_test = np.array(real_x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFR Hyperparameter optimizer through grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RFR_space = {'criterion': hp.choice('criterion', ['squared_error', 'absolute_error', 'poisson', 'friedman_mse']),\n",
    "             'max_depth': hp.choice('max_depth', range(1, 10)),\n",
    "             'min_samples_split': hp.choice('min_samples_split', range(2, 10)),\n",
    "             'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 10)),\n",
    "             'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0, 0.5),\n",
    "             'max_features': hp.choice('max_features', ['sqrt', 'log2', None]),\n",
    "             'max_leaf_nodes': hp.choice('max_leaf_nodes', range(2, 10)),\n",
    "             'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 0.5),\n",
    "             'verbose': 1,\n",
    "             'n_jobs': None\n",
    "             }\n",
    "def RFR_objective(space):\n",
    "    model = RandomForestRegressor(**space)\n",
    "    accuracy = cross_val_score(model, x_train, y_train, cv = 5).mean()\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "    \n",
    "trials = Trials()\n",
    "\n",
    "best_RFR = fmin(fn = RFR_objective,\n",
    "            space = RFR_space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 50,\n",
    "            trials = trials)\n",
    "\n",
    "\n",
    "best_RFR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'criterion' : 'friedman_mse'\n",
    "'max_depth' : 9\n",
    "'max_features': 'log2'\n",
    "'max_leaf_nodes' : 6\n",
    "'min_impurity_decrease' : 0.3345\n",
    "'min_samples_leaf' : 5\n",
    "'min_samples_split' : 5\n",
    "'min_weight_fraction_leaf': 0.000216\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristian\\AppData\\Local\\Temp\\ipykernel_9108\\556813685.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model = regr.fit(x_train, y_train)\n",
      "C:\\Users\\Cristian\\AppData\\Local\\Temp\\ipykernel_9108\\556813685.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model2 = regr2.fit(x_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid searched hyperparameters metrics\n",
      "Mean absolute error = 0.04\n",
      "Mean squared error = 0.01\n",
      "Median absolute error = 0.02\n",
      "Explain variance score = -0.0\n",
      "R2 score = -0.01\n",
      "Z score = 1.490589531982948e-17\n",
      "\n",
      "\n",
      "No hyperparameters metrics\n",
      "Mean absolute error = 0.05\n",
      "Mean squared error = 0.02\n",
      "Median absolute error = 0.0\n",
      "Explain variance score = -0.15\n",
      "R2 score = -0.16\n",
      "Z score = -3.172826289506561e-17\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(criterion = 'friedman_mse',\n",
    "                                max_depth = 9,\n",
    "                                max_features= 'log2',\n",
    "                                max_leaf_nodes= 6,\n",
    "                                min_impurity_decrease= 0.3345,\n",
    "                                min_samples_leaf= 5,\n",
    "                                min_samples_split= 5,\n",
    "                                min_weight_fraction_leaf= 0.000216,\n",
    "                                )\n",
    "\n",
    "model = regr.fit(x_train, y_train)\n",
    "\n",
    "regr2 = RandomForestRegressor()\n",
    "\n",
    "model2 = regr2.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_test_pred = model.predict(x_test)\n",
    "y_2_test_pred = model2.predict(x_test)\n",
    "\n",
    "#save \n",
    "import sklearn.metrics as sm\n",
    "import scipy.stats as stats\n",
    "\n",
    "print(\"Grid searched hyperparameters metrics\")\n",
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_test_pred), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_test_pred), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_test_pred), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_test_pred), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(y_test, y_test_pred), 2))\n",
    "print(\"Z score =\",  np.average(stats.zscore(y_test_pred)))\n",
    "print('\\n')\n",
    "print(\"No hyperparameters metrics\")\n",
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_2_test_pred), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_2_test_pred), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_2_test_pred), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_2_test_pred), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(y_test, y_2_test_pred), 2))\n",
    "print(\"Z score =\",  np.average(stats.zscore(y_2_test_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cristian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM metrics\n",
      "Mean absolute error = 0.1\n",
      "Mean squared error = 0.02\n",
      "Median absolute error = 0.08\n",
      "Explain variance score = -0.0\n",
      "R2 score = -0.22\n",
      "Z score = -1.9335075643435957e-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as npimport\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "regressor = SVR(kernel = 'rbf')\n",
    "regressor.fit(x_train, y_train)\n",
    "svm_y_test = regressor.predict(x_test)\n",
    "\n",
    "import sklearn.metrics as sm\n",
    "import scipy.stats as stats\n",
    "\n",
    "print(\"SVM metrics\")\n",
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, svm_y_test), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, svm_y_test), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, svm_y_test), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, svm_y_test), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(y_test, svm_y_test), 2))\n",
    "print(\"Z score =\",  np.average(stats.zscore(svm_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ANN model\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(layers.Dense(4))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer='rmsprop' metrics = ['accuracy'], )\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0693 - accuracy: 0.1237 - val_loss: 0.2187 - val_accuracy: 0.1108\n",
      "Epoch 2/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0693 - accuracy: 0.1237 - val_loss: 0.2166 - val_accuracy: 0.1108\n",
      "Epoch 3/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2081 - val_accuracy: 0.1108\n",
      "Epoch 4/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0693 - accuracy: 0.1237 - val_loss: 0.2098 - val_accuracy: 0.1108\n",
      "Epoch 5/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2153 - val_accuracy: 0.1108\n",
      "Epoch 6/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2182 - val_accuracy: 0.1108\n",
      "Epoch 7/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0693 - accuracy: 0.1237 - val_loss: 0.2141 - val_accuracy: 0.1108\n",
      "Epoch 8/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0693 - accuracy: 0.1237 - val_loss: 0.2112 - val_accuracy: 0.1108\n",
      "Epoch 9/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2144 - val_accuracy: 0.1108\n",
      "Epoch 10/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2314 - val_accuracy: 0.1108\n",
      "Epoch 11/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0693 - accuracy: 0.1237 - val_loss: 0.2195 - val_accuracy: 0.1108\n",
      "Epoch 12/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2200 - val_accuracy: 0.1108\n",
      "Epoch 13/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2223 - val_accuracy: 0.1108\n",
      "Epoch 14/100\n",
      "3770/3770 [==============================] - 7s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2146 - val_accuracy: 0.1108\n",
      "Epoch 15/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2223 - val_accuracy: 0.1108\n",
      "Epoch 16/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2242 - val_accuracy: 0.1108\n",
      "Epoch 17/100\n",
      "3770/3770 [==============================] - 7s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2227 - val_accuracy: 0.1108\n",
      "Epoch 18/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2236 - val_accuracy: 0.1108\n",
      "Epoch 19/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2085 - val_accuracy: 0.1108\n",
      "Epoch 20/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2150 - val_accuracy: 0.1108\n",
      "Epoch 21/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2188 - val_accuracy: 0.1108\n",
      "Epoch 22/100\n",
      "3770/3770 [==============================] - 8s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2216 - val_accuracy: 0.1108\n",
      "Epoch 23/100\n",
      "3770/3770 [==============================] - 7s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2197 - val_accuracy: 0.1108\n",
      "Epoch 24/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2192 - val_accuracy: 0.1108\n",
      "Epoch 25/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2166 - val_accuracy: 0.1108\n",
      "Epoch 26/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2228 - val_accuracy: 0.1108\n",
      "Epoch 27/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2237 - val_accuracy: 0.1108\n",
      "Epoch 28/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2233 - val_accuracy: 0.1108\n",
      "Epoch 29/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2142 - val_accuracy: 0.1108\n",
      "Epoch 30/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2176 - val_accuracy: 0.1108\n",
      "Epoch 31/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2192 - val_accuracy: 0.1108\n",
      "Epoch 32/100\n",
      "3770/3770 [==============================] - 6s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2075 - val_accuracy: 0.1108\n",
      "Epoch 33/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2125 - val_accuracy: 0.1108\n",
      "Epoch 34/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2223 - val_accuracy: 0.1108\n",
      "Epoch 35/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2145 - val_accuracy: 0.1108\n",
      "Epoch 36/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2177 - val_accuracy: 0.1108\n",
      "Epoch 37/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2275 - val_accuracy: 0.1108\n",
      "Epoch 38/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2192 - val_accuracy: 0.1108\n",
      "Epoch 39/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2184 - val_accuracy: 0.1108\n",
      "Epoch 40/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2212 - val_accuracy: 0.1108\n",
      "Epoch 41/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2216 - val_accuracy: 0.1108\n",
      "Epoch 42/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2133 - val_accuracy: 0.1108\n",
      "Epoch 43/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2131 - val_accuracy: 0.1108\n",
      "Epoch 44/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2341 - val_accuracy: 0.1108\n",
      "Epoch 45/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2163 - val_accuracy: 0.1108\n",
      "Epoch 46/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2122 - val_accuracy: 0.1108\n",
      "Epoch 47/100\n",
      "3770/3770 [==============================] - 6s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2211 - val_accuracy: 0.1108\n",
      "Epoch 48/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2144 - val_accuracy: 0.1108\n",
      "Epoch 49/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2185 - val_accuracy: 0.1108\n",
      "Epoch 50/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2135 - val_accuracy: 0.1108\n",
      "Epoch 51/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2098 - val_accuracy: 0.1108\n",
      "Epoch 52/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2282 - val_accuracy: 0.1108\n",
      "Epoch 53/100\n",
      "3770/3770 [==============================] - 6s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2238 - val_accuracy: 0.1108\n",
      "Epoch 54/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0690 - accuracy: 0.1237 - val_loss: 0.2184 - val_accuracy: 0.1108\n",
      "Epoch 55/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0692 - accuracy: 0.1237 - val_loss: 0.2218 - val_accuracy: 0.1108\n",
      "Epoch 56/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2194 - val_accuracy: 0.1108\n",
      "Epoch 57/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2069 - val_accuracy: 0.1108\n",
      "Epoch 58/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2230 - val_accuracy: 0.1108\n",
      "Epoch 59/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2158 - val_accuracy: 0.1108\n",
      "Epoch 60/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2200 - val_accuracy: 0.1108\n",
      "Epoch 61/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2205 - val_accuracy: 0.1108\n",
      "Epoch 62/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2230 - val_accuracy: 0.1108\n",
      "Epoch 63/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2188 - val_accuracy: 0.1108\n",
      "Epoch 64/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2083 - val_accuracy: 0.1108\n",
      "Epoch 65/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2126 - val_accuracy: 0.1108\n",
      "Epoch 66/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2153 - val_accuracy: 0.1108\n",
      "Epoch 67/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2117 - val_accuracy: 0.1108\n",
      "Epoch 68/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2256 - val_accuracy: 0.1108\n",
      "Epoch 69/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2221 - val_accuracy: 0.1108\n",
      "Epoch 70/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2090 - val_accuracy: 0.1108\n",
      "Epoch 71/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2238 - val_accuracy: 0.1108\n",
      "Epoch 72/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2181 - val_accuracy: 0.1108\n",
      "Epoch 73/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2058 - val_accuracy: 0.1108\n",
      "Epoch 74/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2146 - val_accuracy: 0.1108\n",
      "Epoch 75/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2190 - val_accuracy: 0.1108\n",
      "Epoch 76/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2232 - val_accuracy: 0.1108\n",
      "Epoch 77/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2282 - val_accuracy: 0.1108\n",
      "Epoch 78/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2173 - val_accuracy: 0.1108\n",
      "Epoch 79/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2097 - val_accuracy: 0.1108\n",
      "Epoch 80/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2090 - val_accuracy: 0.1108\n",
      "Epoch 81/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2202 - val_accuracy: 0.1108\n",
      "Epoch 82/100\n",
      "3770/3770 [==============================] - 6s 2ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2168 - val_accuracy: 0.1108\n",
      "Epoch 83/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2286 - val_accuracy: 0.1108\n",
      "Epoch 84/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2232 - val_accuracy: 0.1108\n",
      "Epoch 85/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2234 - val_accuracy: 0.1108\n",
      "Epoch 86/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2202 - val_accuracy: 0.1108\n",
      "Epoch 87/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2220 - val_accuracy: 0.1108\n",
      "Epoch 88/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2249 - val_accuracy: 0.1108\n",
      "Epoch 89/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2274 - val_accuracy: 0.1108\n",
      "Epoch 90/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2192 - val_accuracy: 0.1108\n",
      "Epoch 91/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2179 - val_accuracy: 0.1108\n",
      "Epoch 92/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2162 - val_accuracy: 0.1108\n",
      "Epoch 93/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2199 - val_accuracy: 0.1108\n",
      "Epoch 94/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2150 - val_accuracy: 0.1108\n",
      "Epoch 95/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0690 - accuracy: 0.1237 - val_loss: 0.2165 - val_accuracy: 0.1108\n",
      "Epoch 96/100\n",
      "3770/3770 [==============================] - 6s 1ms/step - loss: 0.0690 - accuracy: 0.1237 - val_loss: 0.2049 - val_accuracy: 0.1108\n",
      "Epoch 97/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2258 - val_accuracy: 0.1108\n",
      "Epoch 98/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2204 - val_accuracy: 0.1108\n",
      "Epoch 99/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2190 - val_accuracy: 0.1108\n",
      "Epoch 100/100\n",
      "3770/3770 [==============================] - 5s 1ms/step - loss: 0.0691 - accuracy: 0.1237 - val_loss: 0.2247 - val_accuracy: 0.1108\n"
     ]
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "history = model.fit(x_train, y_train, batch_size=10, epochs=100, verbose= 1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 0s 643us/step\n",
      "SVM metrics\n",
      "Mean absolute error = 0.04\n",
      "Mean squared error = 0.02\n",
      "Median absolute error = 0.0\n",
      "Explain variance score = -0.09\n",
      "R2 score = -0.12\n",
      "Z score = -4.344237e-09\n"
     ]
    }
   ],
   "source": [
    "nn_y_test = model.predict(x_test)\n",
    "\n",
    "print(\"SVM metrics\")\n",
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, nn_y_test), 2)) \n",
    "print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, nn_y_test), 2)) \n",
    "print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, nn_y_test), 2)) \n",
    "print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, nn_y_test), 2)) \n",
    "print(\"R2 score =\", round(sm.r2_score(y_test, nn_y_test), 2))\n",
    "print(\"Z score =\",  np.average(stats.zscore(nn_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss and Accuracy: [0.06900038570165634, 0.12372665852308273]\n",
      "Testing Loss and Accuracy: [0.22466275095939636, 0.11082474142313004]\n"
     ]
    }
   ],
   "source": [
    "loss_and_accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Loss and Accuracy:\", loss_and_accuracy)\n",
    "\n",
    "loss_and_accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"Testing Loss and Accuracy:\", loss_and_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to find the best parameters for ANN\n",
    "def FunctionFindBestParams(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    #batch_size_list=[5, 10, 15, 20]\n",
    "    batch_size_list=[5,10]\n",
    "    #epoch_list  =   [5, 10, 50, 100]\n",
    "    epoch_list = [5,10]\n",
    "    \n",
    "    import pandas as pd\n",
    "    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])\n",
    "    \n",
    "    # initializing the trials\n",
    "    TrialNumber=0\n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=5, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # The output neuron is a single fully connected node \n",
    "            # Since we will be predicting a single number\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(x_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=0)\n",
    " \n",
    "            error = y_test-model.predict(x_test)\n",
    "            error = np.nan_to_num(error, nan=0, posinf=0, neginf=0)\n",
    "            MAPE = np.mean(100 * (np.abs(error) / y_test))\n",
    "\n",
    "            \n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', 100-MAPE)\n",
    "            \n",
    "            SearchResultsData.append(pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), 100-MAPE]],\n",
    "                                                                    columns=['TrialNumber', 'Parameters', 'Accuracy'] ))\n",
    "    return(SearchResultsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function\n",
    "ResultsData=FunctionFindBestParams(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsData.plot(x='Parameters', y='Accuracy', figsize=(15,4), kind='line')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
